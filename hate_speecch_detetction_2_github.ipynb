{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rF3qOV4O0tLn",
    "outputId": "f181adb5-640e-45ea-e8a6-1d3a37d2b2a1"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Install ftfy for fixing text encoding issues\n",
    "!pip install -q ftfy\n",
    "\n",
    "import pandas as pd\n",
    "from ftfy import fix_text\n",
    "\n",
    "# Load the previously uploaded CSV file\n",
    "input_path = \"/content/drive/MyDrive/test.csv\"\n",
    "output_path_cleaned = \"/content/drive/MyDrive/test_cleaned.csv\"\n",
    "\n",
    "# Read CSV (attempt auto-detection of encoding and fix common issues)\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# Function to clean text: fix encoding, normalize whitespace, lowercase\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        fixed = fix_text(text)  # Fix mojibake and encoding\n",
    "        fixed = fixed.lower()   # Lowercase\n",
    "        fixed = ' '.join(fixed.split())  # Normalize whitespace\n",
    "        return fixed\n",
    "    return text\n",
    "\n",
    "# Apply cleaning to all string columns\n",
    "df_cleaned = df.applymap(clean_text)\n",
    "\n",
    "# Save cleaned CSV\n",
    "df_cleaned.to_csv(output_path_cleaned, index=False, encoding='utf-8')\n",
    "\n",
    "output_path_cleaned\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "xsdjjGPu08mi",
    "outputId": "414eb3e5-1765-4d87-88d3-c657a5fc35bf"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-3-76764b897cbf>:24: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_cleaned = df.applymap(clean_text)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/content/drive/MyDrive/test_cleaned.csv'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 3
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/hate/data/test_cleaned_final.csv\")\n",
    "# Show basic info\n",
    "print(df.columns)\n",
    "print(df.head())\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uHtCxMjt5I5P",
    "outputId": "2f8e546c-58d0-4b5c-f6bd-be3484632275"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Index(['Offensive', 'Non-Offensive'], dtype='object')\n",
      "                                           Offensive  \\\n",
      "0  i don't like this situation, you fat cow, just...   \n",
      "1    that wasn't very kind, you shithead, just stop.   \n",
      "2  everyone deserves respect, you retard, just stop.   \n",
      "3    we should try to fix this, you jerk, just stop.   \n",
      "4        that wasn't very kind, you slut, just stop.   \n",
      "\n",
      "                                     Non-Offensive  \n",
      "0  i don't want to talk like this. let's cool off.  \n",
      "1   you're not helping the situation, so back off.  \n",
      "2  i don't want to talk like this. let's cool off.  \n",
      "3             let's end this conversation for now.  \n",
      "4             let's end this conversation for now.  \n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Rename columns for clarity\n",
    "df = df.rename(columns={\"Offensive\": \"original_text\", \"Non-Offensive\": \"neutral_text\"})\n",
    "\n",
    "# Drop unused columns\n",
    "df = df[[\"original_text\", \"neutral_text\"]].dropna()\n",
    "\n",
    "# Preview cleaned data\n",
    "df.head()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "d00fdF_S5mlr",
    "outputId": "5cac52a9-43df-4dab-ae1c-6bbf49ca35cb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install transformers datasets sentencepiece --quiet\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yweORMqu5qNn",
    "outputId": "675c4917-3f04-4752-9ffa-16a83e466002"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load cleaned data\n",
    "#elects only two columns: Offensive (original) and Non-Offensive (target)\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/hate/data/test_cleaned_final.csv\")[[\"Offensive\", \"Non-Offensive\"]]\n",
    "df = df.rename(columns={\"Offensive\": \"original_text\", \"Non-Offensive\": \"neutral_text\"}).dropna()#Removes rows with missing values in either column\n",
    "\n",
    "df[\"original_text\"] = \"rephrase: \" + df[\"original_text\"]\n",
    "\n",
    "\n",
    "dataset = Dataset.from_pandas(df[[\"original_text\", \"neutral_text\"]])\n",
    "dataset = dataset.train_test_split(test_size=0.1)  # 90% train, 10% test\n"
   ],
   "metadata": {
    "id": "BjvTiOB05r4V"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "#5-small (60M params) - balanced between performance and efficiency\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"original_text\"], padding=\"max_length\", truncation=True, max_length=128)#Fixed length 128 tokens\n",
    "\n",
    "def tokenize_labels(batch):\n",
    "    labels = tokenizer(batch[\"neutral_text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    batch[\"labels\"] = labels[\"input_ids\"]\n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(tokenize)\n",
    "dataset = dataset.map(tokenize_labels)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345,
     "referenced_widgets": [
      "ec164d108d494ac8b9056105079bd387",
      "ec690adafc48460faaac45a37dd13fbb",
      "e6b14e35e0854cb8bae148114be2ad6e",
      "7d4c7c246b3e49c083a289bd2c8ea3bc",
      "cb299bcbb03442638ecbca331ed57f08",
      "4ae4f1f2e4aa4c129804e8a6a32e7b27",
      "c29aa5d9038e4ad58d824ed97f49fcef",
      "02406ed9a1c84449b11f2fcc0ef6cad3",
      "eae929375699464d89bc3b1c2d4f7065",
      "a57751e68a0a443cae79d4a49becbbf5",
      "4f3585618b944431aba27c5130865875",
      "05392c5b6dd74c51870a58d98921e887",
      "6c8c0cd729be45eaae8068ad225e7b28",
      "ae6fa705ca934ceabd20dafdffd0612e",
      "2ae0d0948ea64a1aac25ab3f9c0293a2",
      "084c8b0af7544d22863939322f1aef9a",
      "54892f9291624b2f8778c6944d751b8c",
      "f8fa095ba32f4857bf3bdf8ba18ea021",
      "00e4a046dac141ab80053d3561954a3d",
      "9594922cc9ee4570a255db01208768d9",
      "0e48a5b17f034e8e8fa8b4422ce72d23",
      "69a8bc250aae40a9b434453df8fba770",
      "502e8d9d39f343f9916eeb1b69a024f8",
      "9d7036f410c5415fad7f3c04edd18018",
      "c024ecc20d904d41bfa320d8ce520d6e",
      "1e244d48aeea42a087987851b9df818b",
      "9900ef4a8b964db9abeb687cc7922ac1",
      "6eb430dde6b84de48c32338e857d2a33",
      "a759d132e2f44d6895355fc9d1c33327",
      "5fcde05a8e3a40ffb3ae97c4516c97d4",
      "6350978f9fc546caa9a7e00a7b36262c",
      "619c877af28f4d39a7e1c26d0e6dec88",
      "4ee8edbdb9e24adaa8016a07ed577943",
      "dce42977aa5c4916886a753e9998d650",
      "305ea6ea4f6e4bf19b829bf75181b9c9",
      "2a5b66f453ee4c74a6e13e5ab413ab76",
      "3c129c714929449cbe50761f0e06a594",
      "fa39a15a502242b48e7b6f8c54e26673",
      "ee1070107f1247619db6534725ddc55e",
      "c099fd062d094e669c4414a8131fa5d8",
      "d5cda9c48b8a499082316ec8de0f6378",
      "27f0c636564b42a9b0e4a67728fe9d7d",
      "ee8c4fbbedf34d1789013eaa4d0ec9d1",
      "a928c1211953499fb9c078bc07204a90",
      "b72f323ae3d34806bf6ef188d08e8a34",
      "3b0c3c67f41944debc55454450204b69",
      "de970962f81245f7899fa688e0451614",
      "bc22afa1f33c4cbf86ead374f590d506",
      "4a56882237ff45edb948fb326a0ef1d2",
      "a2be88076f1c4419b0ce183e50b949a1",
      "597381a92eb841caa46ecbea9ef2721c",
      "b92ae94f03a24100b19d44ada1ca7fdc",
      "0503d882d2f54576a0465d738de1b227",
      "6a7ea8b9034d4fbca7f94e499c48f303",
      "7653a337e03344beb86254ce888a7b74",
      "89dfd89239fa4a668bdf4785581a0f9f",
      "085aa5ca8c4a421a8c7ca0bef8cb692d",
      "cc2ff44578c9419caa10a684111d1cc3",
      "5486442f34bf4c8581b6f00d98c296ef",
      "8b37b2ada6b84bf2bba279866d0e3b97",
      "251599564658438bb90608a5463cc0d6",
      "925b6e551e9a4862a661515c6c9d7ced",
      "760203ef068448eabd830846b6165b50",
      "c28b686805624585addcd4334c1d18b3",
      "6a139628849142b7b31b6cb7adcf513f",
      "f013d0d2bb434962b0e751b492a899ae",
      "a0583cd8c6f54117a489004a782daa73",
      "c6fa3d8f828b44439605b2e96772eec3",
      "b86806a580d84169a0db7290f3e26234",
      "67ad41f51d4642e0bebd98d1a7b0bc47",
      "fca0cfc9d48d4b8e80bb1a1b201f9bcd",
      "72aaa2bc4d264bcc9cb0617865ae36cf",
      "a7976c5184314acea72392c575718011",
      "0e336de149004257b721671058c15065",
      "c524782a60db4042838993f9df64cf87",
      "dd7d73a75a8345e2ac0b40ae55cf9c65",
      "0e01007301d54920b2216ab797a3de3b"
     ]
    },
    "id": "WugIERcm56GT",
    "outputId": "0f9432c0-39f4-42ac-c08c-2991e8b27c3f"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ec164d108d494ac8b9056105079bd387"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "05392c5b6dd74c51870a58d98921e887"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "502e8d9d39f343f9916eeb1b69a024f8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/9000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dce42977aa5c4916886a753e9998d650"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b72f323ae3d34806bf6ef188d08e8a34"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/9000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "89dfd89239fa4a668bdf4785581a0f9f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a0583cd8c6f54117a489004a782daa73"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, TrainingArguments, Trainer\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "#Encoder stack (6 layers)\n",
    "#Decoder stack (6 layers)\n",
    "#Classification head for token prediction\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./t5_rephrase_model\",  # Model checkpoints save location\n",
    "    num_train_epochs=5,               # Full passes through dataset\n",
    "    per_device_train_batch_size=4,    # Batch size per GPU\n",
    "    per_device_eval_batch_size=4,     # Eval batch size\n",
    "    eval_strategy=\"epoch\",            # Evaluate after each epoch\n",
    "    save_strategy=\"epoch\",            # Save checkpoint every epoch\n",
    "    logging_steps=10,                 # Log metrics every 10 steps\n",
    "    learning_rate=5e-5,               # Peak learning rate\n",
    "    weight_decay=0.01,                # L2 regularization\n",
    "    save_total_limit=2,               # Max checkpoints to keep\n",
    "    load_best_model_at_end=True       # Keep best model (by eval loss)\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                   # The T5 model\n",
    "    args=training_args,            # Configuration\n",
    "    train_dataset=dataset[\"train\"], # Training data (tokenized)\n",
    "    eval_dataset=dataset[\"test\"],   # Validation data\n",
    "    tokenizer=tokenizer            # Same tokenizer for decoding\n",
    "\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504,
     "referenced_widgets": [
      "aba676ed49114767964a23fb5e6b01b1",
      "a5cf289b4cb741d6a2ac985afe5be8af",
      "91b46dd6e7a848b1a478125996445534",
      "32e0f0763591428d9593398f780e0528",
      "c193550e95e046ceb357d8fc55a77741",
      "7cb34310758e43ac9a056831db95c5ba",
      "c2dc20417a3948bc9e9e575b149b3d57",
      "8fc04191baae4794a7108de292d2099e",
      "930675692a534fec85e7a3f7e744c493",
      "343309538b8a44349b5074bb67e29a1e",
      "944588b0e3a345d4a26f1e73844ab980",
      "241d192c84df402c8c56d2ee754617a5",
      "ca1cbf1949da4ae7abf13a07df59b84f",
      "47bdf8e5639d4e7ab8f380e85ae56b23",
      "e0b2209c887e452d9f871ec94340b12e",
      "a29dfdc523634537885cbc1330f7b5d7",
      "68d96b920a554a9bb1d4537a134c8f52",
      "5ee45df17d4945fca1a17b72cd0ea132",
      "8bc36f9b96914e1babf031e09d126572",
      "5d90f658676243e4b00a6a4daf4b24d2",
      "4379df10d31f4a7586ed23dd5c3467a9",
      "1554cea235b148e580772e243492d5d5",
      "e30e291930c14e31b469de13dd65a68a",
      "6415640c5aa841f3b2a2c9930e86b914",
      "18638222c5ec47219a69e0ad32e4b19b",
      "6066682d54564083a4359b34710861e8",
      "e6c14319c94f417aa606ce091b905b87",
      "b28dc81abfef455ebe6d7d18ad248d24",
      "4061725db03f4808bf606f7fa74018a8",
      "ee14119fb1c7454ea1961505c8530880",
      "adc030dbfd264a1b98cab55586e4ec65",
      "34715238fbec4dd6b2d09cfe7d1d2a3a",
      "fea54bdcf9d44a3fbb7991741674ec2b"
     ]
    },
    "id": "c1rkvaP559v0",
    "outputId": "ab6d8a3f-55cd-4b70-fb07-3e3a08ed9580"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aba676ed49114767964a23fb5e6b01b1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "241d192c84df402c8c56d2ee754617a5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e30e291930c14e31b469de13dd65a68a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "<ipython-input-8-de741f3d0c40>:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainOutput(global_step=11250, training_loss=0.041906213309036364, metrics={'train_runtime': 1182.6331, 'train_samples_per_second': 38.051, 'train_steps_per_second': 9.513, 'total_flos': 1522595266560000.0, 'train_loss': 0.041906213309036364, 'epoch': 5.0})"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "trainer.save_model(\"/content/drive/MyDrive/hate/t5-rephrase-model\")\n",
    "tokenizer.save_pretrained(\"/content/drive/MyDrive/hate/t5-rephrase-model\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_qBIC341AMFS",
    "outputId": "9cb144c5-7254-46e3-e545-607b3d28d6e4"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('/content/drive/MyDrive/hate/t5-rephrase-model/tokenizer_config.json',\n",
       " '/content/drive/MyDrive/hate/t5-rephrase-model/special_tokens_map.json',\n",
       " '/content/drive/MyDrive/hate/t5-rephrase-model/spiece.model',\n",
       " '/content/drive/MyDrive/hate/t5-rephrase-model/added_tokens.json',\n",
       " '/content/drive/MyDrive/hate/t5-rephrase-model/tokenizer.json')"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#T5 output\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/hate/data/test_cleaned_final.csv\")\n",
    "df = df.rename(columns={\"Offensive\": \"original_text\", \"Non-Offensive\": \"neutral_text\"})\n",
    "df = df[[\"original_text\", \"neutral_text\"]].dropna()\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_path = \"/content/drive/MyDrive/hate/t5-rephrase-model\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "model.eval()\n",
    "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Prepare inputs\n",
    "BATCH_SIZE = 16\n",
    "inputs = [f\"rephrase: {t}\" for t in df[\"original_text\"].tolist()]\n",
    "all_preds = []\n",
    "\n",
    "# Batched inference\n",
    "for i in tqdm(range(0, len(inputs), BATCH_SIZE)):\n",
    "    batch = inputs[i:i+BATCH_SIZE]\n",
    "    encodings = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    input_ids = encodings[\"input_ids\"].to(model.device)\n",
    "    attention_mask = encodings[\"attention_mask\"].to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=128,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    decoded = [tokenizer.decode(o, skip_special_tokens=True) for o in outputs]\n",
    "    all_preds.extend(decoded)\n",
    "\n",
    "# Heuristic label function (same as before)\n",
    "def categorize(text):\n",
    "    text = text.lower()\n",
    "    if any(w in text for w in [\"n****\", \"chink\", \"race\", \"ethnicity\"]):\n",
    "        return \"Hateful\"\n",
    "    elif any(w in text for w in [\"retarded\", \"idiot\", \"hoe\"]):\n",
    "        return \"Offensive\"\n",
    "    else:\n",
    "        return \"Neither\"\n",
    "\n",
    "def categorize_pred(text):\n",
    "    text = text.lower()\n",
    "    if any(w in text for w in [\"race\", \"ethnicity\", \"n****\", \"chink\"]):\n",
    "        return \"Hateful\"\n",
    "    elif any(w in text for w in [\"respect\", \"tone\", \"calm\", \"inclusive\"]):\n",
    "        return \"Offensive\"\n",
    "    else:\n",
    "        return \"Neither\"\n",
    "\n",
    "# Assign labels\n",
    "y_true = df[\"original_text\"].apply(categorize).tolist()\n",
    "y_pred = [categorize_pred(p) for p in all_preds]\n",
    "\n",
    "# Evaluation\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print(f\"\\nAccuracy: {acc:.2f}\\n\")\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_true, y_pred, digits=2))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[\"Hateful\", \"Offensive\", \"Neither\"])\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\",\n",
    "            xticklabels=[\"Hateful\", \"Offensive\", \"Neither\"],\n",
    "            yticklabels=[\"Hateful\", \"Offensive\", \"Neither\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 708
    },
    "id": "W9O6nY0vAaUz",
    "outputId": "f9f3dd5e-e1ee-408b-bede-d853012706c1"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "100%|██████████| 625/625 [02:27<00:00,  4.22it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Accuracy: 0.92\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Neither       0.94      0.98      0.96      9371\n",
      "   Offensive       0.00      0.00      0.00       629\n",
      "\n",
      "    accuracy                           0.92     10000\n",
      "   macro avg       0.47      0.49      0.48     10000\n",
      "weighted avg       0.88      0.92      0.90     10000\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install gradio transformers --quiet\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lcyc20SNCM1e",
    "outputId": "e3f24562-ff93-4a01-f804-40123b2bb0b5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import gradio as gr\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load hate speech classifier\n",
    "print(\" Loading classifier model...\")\n",
    "clf_model_path = \"/content/drive/MyDrive/hate/roberta-hate-speech-model-20250501T060330Z-001/roberta-hate-speech-model\"\n",
    "clf_tokenizer = AutoTokenizer.from_pretrained(clf_model_path)\n",
    "clf_model = AutoModelForSequenceClassification.from_pretrained(clf_model_path)\n",
    "print(\" Classifier loaded!\")\n",
    "\n",
    "# Load rephraser model\n",
    "print(\"Loading rephraser model...\")\n",
    "reph_model_path = \"/content/drive/MyDrive/hate/t5-rephrase-model\"\n",
    "reph_tokenizer = AutoTokenizer.from_pretrained(reph_model_path)\n",
    "reph_model = AutoModelForSeq2SeqLM.from_pretrained(reph_model_path)\n",
    "print(\"Rephraser loaded!\")\n",
    "\n",
    "label_map = {0: \"Hateful\", 1: \"Neither\", 2: \"Offensive\"}\n",
    "\n",
    "def classify_and_rephrase(text, rephrase_option):\n",
    "    # Step 1: Classification\n",
    "    inputs = clf_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = clf_model(**inputs)\n",
    "        probs = F.softmax(outputs.logits, dim=1)\n",
    "        confidence, prediction = torch.max(probs, dim=1)\n",
    "\n",
    "    label_str = label_map[prediction.item()]\n",
    "    confidence_pct = round(confidence.item() * 100, 2)\n",
    "    prob_str = \", \".join(f\"{label_map[i]}: {round(p * 100, 2)}%\" for i, p in enumerate(probs[0].tolist()))\n",
    "\n",
    "    response = f\" Prediction:{label_str}\\t Confidence: {confidence_pct}%\\t Probabilities: {prob_str}\"\n",
    "\n",
    "    # Step 2: Optional Rephrasing\n",
    "    if rephrase_option and label_str in [\"Hateful\", \"Offensive\"]:\n",
    "        reph_input = reph_tokenizer(f\"rephrase: {text}\", return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        reph_output = reph_model.generate(\n",
    "            **reph_input,\n",
    "            max_length=64,     # Reduced for speed\n",
    "            num_beams=2,       # Reduced for speed\n",
    "            early_stopping=True\n",
    "        )\n",
    "        rephrased_text = reph_tokenizer.decode(reph_output[0], skip_special_tokens=True)\n",
    "        response += f\"\\n Rephrased Text: {rephrased_text}\"\n",
    "\n",
    "    return response\n",
    "\n",
    "# Launch Gradio UI\n",
    "gr.Interface(\n",
    "    fn=classify_and_rephrase,\n",
    "    inputs=[\n",
    "        gr.Textbox(lines=3, placeholder=\"Enter a sentence to classify...\"),\n",
    "        gr.Checkbox(label=\"Also rephrase if hateful or offensive\")\n",
    "    ],\n",
    "    outputs=\"markdown\",\n",
    "    title=\" Hate Speech Classifier + Rephraser\",\n",
    "    description=\"Classifies text as Hateful, Offensive, or Neither. Optionally rephrases hateful/offensive text into neutral language.\",\n",
    ").launch()  # Remove share=True for speed\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 695
    },
    "id": "30SB0pMbB-mx",
    "outputId": "143e4f2e-4503-437c-d464-515b99ac4e16"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " Loading classifier model...\n",
      " Classifier loaded!\n",
      "Loading rephraser model...\n",
      "Rephraser loaded!\n",
      "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://50c5159cf6a9917ef2.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div><iframe src=\"https://50c5159cf6a9917ef2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "execution_count": 5
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install transformers datasets scikit-learn\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vCgxiVctUqIz",
    "outputId": "bcb40258-d714-4e0a-fea2-382d661242f1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from collections import Counter"
   ],
   "metadata": {
    "id": "df7ruCflUh5S"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"Hate-speech-CNERG/hatexplain\", trust_remote_code=True)\n",
    "\n",
    "# Convert list of tokens into full text\n",
    "def tokens_to_text(example):\n",
    "    return {\"text\": \" \".join(example[\"post_tokens\"])}\n",
    "\n",
    "for split in dataset:\n",
    "    dataset[split] = dataset[split].map(tokens_to_text)\n",
    "\n",
    "# Get majority vote label from annotators\n",
    "#Problem: Each text has multiple annotations (3 annotators)\n",
    "#Solution: Uses majority voting to create single labels\n",
    "#Label Mapping:\n",
    "#0: Normal\n",
    "#1: Hate speech\n",
    "#2: Offensive\n",
    "def get_majority_label(example):\n",
    "    label_list = example[\"annotators\"][\"label\"]\n",
    "    majority_label = Counter(label_list).most_common(1)[0][0]\n",
    "    return {\"labels\": majority_label}\n",
    "\n",
    "for split in dataset:\n",
    "    dataset[split] = dataset[split].map(get_majority_label)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 557,
     "referenced_widgets": [
      "043eb271240447c1ba33d5f10f655c71",
      "122540b6f20c4f818e549b473eaf5fd8",
      "418f725bc9ba4de4818bcf6b7e91ff42",
      "b1545b1036f04ea4b6ca7457e33c6b44",
      "446336b3d0484a8e8eaa3da37eb1668c",
      "28c02d7d1f3647e6a728c6913efb55c8",
      "7336a924cddf4e5ea8f3ae8059eb46cb",
      "45cb0f099f52461b93a9792bf740eae7",
      "11f4b313904b42e48b8d11d487d86d4b",
      "bbb855811c644ca489b28ecb237c71cc",
      "d508844b188f45d1bc3aedb1049e1332",
      "3008dc0d736940078d20c4916d334b62",
      "88be9e9131db40678f7ae4c5592a547b",
      "f15519a9d0fa4b548f16875816a24c76",
      "ed5bab47f4634b0bb80aab44cd2d5143",
      "f5113c39cf864fb3a07a88a95e6ad9b8",
      "afaeeeeb31c74aa8801711f31d9ef3bc",
      "6d9edbbe782849e8935d02f355da4341",
      "e5dcefd47165405db69a03960a5229d9",
      "f2088eddf1924bb8af0e24f8fc6adbc3",
      "46d22cccb7a04773b300008bd365d4e6",
      "0ebc06c5307243a3bebb89c6ef60aa65",
      "3ec6a8fc81234aeaa8ba076bc5b7c3e0",
      "721975910f0f405288e4c076b19bb77a",
      "a7b86e101e9440c283dd8569c043a74b",
      "a1df2c33543442138b41ba661cb22fac",
      "2265822e9be44db0b576b9e505670bf8",
      "7114f4f247e24a28a46162463089f09a",
      "474cc2553df14657adce8a72fdf36a7e",
      "83858d145e8e49f2afcd8f0304d2b16e",
      "46f622edb5ff4618a919d6c4031edcfa",
      "cd8908de006840b4ae01ef384ecc6721",
      "258cf49fd7334d1eb120c23aa8b1d443",
      "02bf2c715d274bf6ada4c9c2728bfb8d",
      "3b6d2913c48e4b878096aa314a67b24a",
      "8dfe08fd3d454cbe8ba9c4e5e971aada",
      "abf8a59d593a4fbeb535e3cba3a4396f",
      "cddafbdd1cb84138a0e2691ca79d5513",
      "7a880979102345d4a1accd7c6d44226f",
      "91c587df6ec943c4ae8868a07d03bfce",
      "0075e81ce33d48eebe348066c13737b8",
      "f964dcf96d8c4e9a91e3cc676dd661eb",
      "6344aca2eb4449c99f181a46832da953",
      "9e69f2a548e145e29d06aede62221f78",
      "b0792ee4b9544c15978e233b91a74b59",
      "dea93636baac413eba923922fd58a6c5",
      "0146eb3f74054ca98a2fd989d3d7b614",
      "be17918a71a14d4a8764a26089283d7f",
      "90771239d9b14b19b50afe7246e3603f",
      "f182e65dac81460285f14a5e6116b274",
      "96adac40980c4571924911ec46028763",
      "eacc7e24ce094a6585f79488d13ed90f",
      "2fef90fa973d4c9f8dd9b7c6d7ef7595",
      "1970ec82948347308c31d8cce61ad512",
      "45745c30ac1b4e64875b64207bbb9e33",
      "916f1957b36a4ffdb1b2678dfc6c1b1f",
      "8619c9b72edb4806bf8a6c811e334011",
      "5935fe9838ff44b3ba307d875c0e09db",
      "372c910ba3e3491486750cca38da95ce",
      "54265c231d5c498fb76892f242b9d2ba",
      "faa7d0d9df4448ee8e9523acff3ef156",
      "dfec571e0aa04b8f832bcc78c31aa79b",
      "203555e911864adb9453cda77a873493",
      "91d6243ccf614b05be7b626fb67a2aaf",
      "6e66265e6aa64f87b074b975683e51d9",
      "749b2768526e47409d1cc933b282b329",
      "e704360d00864891a5574fd227e404a0",
      "c9f6da7c90674ee7a174264b38ad61f7",
      "b71e5a46cb7148058316c5769e971f3c",
      "42cf3b0bb70f4ce2937a6456395273a4",
      "e72c58d2492d4da789fcbb4b9aed9c6f",
      "5f1a92d66c5c4c2b986af3b78aba713c",
      "e052f32ca8744c4bbf0e41bff14857a7",
      "ecad2331665c4ffdbe675bda0ca3ab5a",
      "d206bea7a55e4a3e86c325af7812a3af",
      "9d2e3e6a5dbb45acb80f98c940a67c32",
      "559c1beaaa9c47a197f5183290944a0d",
      "f223f849fb7642fdb705e48032f79e97",
      "48d05c404ff344c8bcf1b21118845bde",
      "b0c5d4badc4d49b491b233c664cc2851",
      "75001135e15743a482389d6d422eb01f",
      "a123a018772b4895a6b052faedc8f9fc",
      "300d24042115407eafe49909f2cf7824",
      "7d1914fe4d96498ebf8dbae3f61dc2e7",
      "b390f803fc2247469892e6c7349e79d5",
      "0192e2397dcc46b5ba3b6f9a9b888d39",
      "f96d96f689ff488bbed23413e14fba77",
      "bf2c09f872bc46958775b80304dbf268",
      "dd09b054a6a542618724330f00f33d55",
      "fb09421af0b143ef9e694ef7307b280d",
      "c396942985024c46be7f675ea2f6e394",
      "843492da374444a7a0f7db4f5d39a12f",
      "012a26b709784fd8aeac39ef5c4e5a7c",
      "5b592ede5d1040c9be8259a13f226e49",
      "86c655349e71432781b17c36745efffb",
      "a38dbe9768fc49fbbd20b1ad90c0cc31",
      "731eb4a1a4b24516bc91a3ed42e4cbfc",
      "e86bed2b36cf4f869a0bb3b781a81bb4",
      "f3b5048f6aa440ca8d89515eb1b35869",
      "a6e6b14bbb214b7697f6af258f011218",
      "15377cee5d7b43d6937b2de6ab127bce",
      "1b0f044757ab4dfd8007d002b2525a99",
      "d12b62a418fc49c3b913d8574b39d23a",
      "e3dc725512514a2297ddcec16027df8f",
      "c091534351454417b95bb8dca63cdca3",
      "0a1f49fd55f24ad380b2122d98439586",
      "11d5f161cb2d4c479f7e3036f639c01c",
      "40f3d330683649fab8ab6630315eb396",
      "834159789c5b421d900b058a0648588f",
      "a35da16d13bc49fda304e8f41de164f9",
      "6f9b148d99204abf8c4b0922e97fb2ec",
      "1063c7bf2b224bf7b892fa0fc9352164",
      "963668439e3442aa8875d2c85457e9c7",
      "a4cb76eb766445afbec05046c7e8039e",
      "0519593e844f4f7eb652df6f89b5212a",
      "ddb14557a3e44774a1860a2ba753191e",
      "619a8688d3af407796325dfdf50bada6",
      "0da86c59377c447eb7be97c0e98e1723",
      "fd157e6c241d41119a4078e3d76673d8",
      "cd48394bdcee435eb1d6b3550bc196ee",
      "eefdb7fb09b7420abde68797b4219881",
      "f6dfce02533b4810af9d975b3e82617d",
      "ac3e871753f24a5b9a25d46f6957e9aa",
      "e8b0be6189bb4a358b626136913d5b3c",
      "3a96172829e34841a5ff387229996175",
      "bc4f296358b941bc95c21d577f964634",
      "a6db4500640a4f0680a3ab81753ced36",
      "dbda24d36b4d4f98be0de8c118906589",
      "e957c73dd3d04f46b1271f722eec6036",
      "d222833bc0904cbcb42e3cad5c5219ea",
      "10e51ad5f146438cb5dca0c523b0a1e7",
      "2dbabcbef4d44dce924f51a1779f7141",
      "63e5b2ba5d3847be87b4ed0646f66ee3",
      "ea8107e10b8d4ab28b4f1c9cdd9eca9c",
      "ad449cda22e74f248fd99deacb47a2b9",
      "81b3b852be2843698715b9e2e47727f3",
      "b5384dae90594def8581e89c3600fa50",
      "6d3fb08594794fc7af32898ffb556692",
      "25792088fa074aea908464b1d1bfd5bc",
      "31d907e08ab44e508d5a231c8e7104d7",
      "a45bd17a7c594752a8f8d0b3fa0ebf9f",
      "a75e529ef4d7447ebd0b698bb950e11a",
      "a0d27564c4ee4a8cbfcc050e99ac6607"
     ]
    },
    "id": "ngIoMzAvUjN2",
    "outputId": "0f83d7f4-513d-40a7-aab3-7e2255793ca9"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md:   0%|          | 0.00/10.1k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "043eb271240447c1ba33d5f10f655c71"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "hatexplain.py:   0%|          | 0.00/4.78k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3008dc0d736940078d20c4916d334b62"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading data:   0%|          | 0.00/12.3M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3ec6a8fc81234aeaa8ba076bc5b7c3e0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading data:   0%|          | 0.00/592k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "02bf2c715d274bf6ada4c9c2728bfb8d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/15383 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b0792ee4b9544c15978e233b91a74b59"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating validation split:   0%|          | 0/1922 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "916f1957b36a4ffdb1b2678dfc6c1b1f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating test split:   0%|          | 0/1924 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e704360d00864891a5574fd227e404a0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/15383 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f223f849fb7642fdb705e48032f79e97"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1922 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dd09b054a6a542618724330f00f33d55"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1924 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a6e6b14bbb214b7697f6af258f011218"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/15383 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6f9b148d99204abf8c4b0922e97fb2ec"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1922 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f6dfce02533b4810af9d975b3e82617d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1924 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "63e5b2ba5d3847be87b4ed0646f66ee3"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Load tokenizer and model (Vanilla BERT)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True,#Cuts texts longer than 128 tokens\n",
    "                     padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", #1 = Real token 0 = Padding token ([PAD])\n",
    "\"labels\"])\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "eval_dataset = tokenized_datasets[\"validation\"]\n",
    "test_dataset = tokenized_datasets[\"test\"]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241,
     "referenced_widgets": [
      "69cc51efc4884ee3a2faa60e9a0c5057",
      "96cbcdc15d6f47d08ba1df5999b54b23",
      "0680ef3fe8cc4923a5d4d48c1d14af61",
      "564f6331e6534dafb489a48ab09358ae",
      "11e1b2c08e0f40ff8e7af89d066a8387",
      "e67343ddd6d7476397a8d7f0ed90a49c",
      "fdee4c3d862d48d6ac99e6e81a2f9f96",
      "29e88bceaa814a53af64df11bc7ab14f",
      "07a025b21c2c49d18788d1ab91d01ef0",
      "1778f45d4eb040389e8999db1b3a921f",
      "541d0548d06a4d64a17c12603fca555c",
      "4877d777d14e46c8a4e49f3c84991b42",
      "1390dcfcc3aa4a29a2b71cd96436bce5",
      "9c845482a88b4f3eaba570a8bc299a69",
      "8dab3e77a40a42728625e088e5728e92",
      "b941f50cb0564dff99b65041ab225cbd",
      "ce863cfcbe5f4e57a9aaa79025f3d932",
      "0745d2847d684b01b6602d0ffe338e17",
      "af785317447249e1825f47a5cd2599ac",
      "c4f5e496420b40ee8bf86434a5127095",
      "b874d1374e8542198510f520b1460dba",
      "6632b96811d144f6a55577fd30c8fd50",
      "c6e4bc9ad88747c68f00c0360cd7a9c0",
      "20587d43fbd344c79b38c44c02e301b0",
      "fff0398e42e546368713ff6656ae988b",
      "3013750136e4468e9de44eaed36bf0e0",
      "c1933d10e8ea4a3ca9a35d35e8e374ba",
      "e21c9738189f45be9da02df0c2591d4d",
      "93e70ed088134d20a1c95c96b1e24f43",
      "c5918392635346288850cdc1c7c266f1",
      "844c27ba87ec47bc8370f9075d9a5744",
      "293e4be959ca4774a1507263dd4b2c21",
      "d1dbb76c53134422bf1591de8eb397ac",
      "eb9b1a2490974a1b9eb7031f7a0de62a",
      "eebda2886ef348e9bc98c82a33926011",
      "b19ae3c31dc746e1900c0d5560968f75",
      "c7a31202aaa04975a8babd43fc5e8e95",
      "5299961993e04809a2921e9b22eae07d",
      "4c006a9102324b94bc066f821386859b",
      "676e3ad429df467a940bd0eb0707a17e",
      "7ec301e7f3074dd5ae5fa0ab6f11d6e5",
      "71472b8b3df54856a4d2e60ef150a3d3",
      "5c172e1f606e4b85bf973fa2f93a1c1d",
      "10278581704f471cb4becef585115ecb",
      "cd54ff2fc9e94a9182aa5cede3a797c9",
      "caceb7f7bdb64f3e801d1242c63fa54f",
      "40b1a3791e8a47f2aef02f690a34c57c",
      "454881ab6bcf42b09494ac96d19e58b1",
      "608247be9ccb477895843a5f47bb3f5b",
      "dce736d328bf46639d6693adafab8ad3",
      "8995446f9866429d834055663250c533",
      "27ae31c88382454d95f7ffad4c6775c8",
      "be1f6e56dfdb40e18566c1c4dfd00189",
      "1b62c6bbff8548a98d1f471c42e951a5",
      "53a2fde68e5443738e4c2196b78bd63c",
      "6138053ae2574bef9cd3bcb3250ff598",
      "cb4bf793f7904a97880beea7c9bdbb4f",
      "1d4c92a98aee4065bd27b0a03ac3f620",
      "86afcc4720f64678b82c31ed8572646e",
      "6fd1149250624f4a9fd9d6d4c1667de4",
      "bb9e516a6d76493ebadf8ff7a3b8225f",
      "45ccb0e9308b449196204451c7f4a198",
      "8e55620a914a4c3399af3fa7be70b1dc",
      "3c9b8b3c110c4e05aeab81861fe63098",
      "9c68f788c5e54a90bfe0b8672461ba1f",
      "6133ae4f33d14e6f931198c5dcb2323f",
      "cec268b3f1384eb196f9dc6852f35c1e",
      "f1ffbaf368344f9c866eb0390e012bac",
      "1316635bc95f4ddca791273a4c74a647",
      "bf330d9ee0c14b788ceafd8729e1db64",
      "5c5fce4773a44bcfaaa7171c4c401e7f",
      "9115adfe9d3e463d933acd17fcd1feb0",
      "0bbd6c7aa55148209fd3dfe4d0369152",
      "92f472239d7d45c1b5f2b984dabaf611",
      "cc9a14abafa1488a990d0c3c5537d7e6",
      "b34bc902bf794637a522501141423489",
      "eb8e26e1c65d420c945088376e3bcac4"
     ]
    },
    "id": "4j2HQDXKU310",
    "outputId": "259b510e-101f-4538-ae98-86657cba6efc"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "69cc51efc4884ee3a2faa60e9a0c5057"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4877d777d14e46c8a4e49f3c84991b42"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c6e4bc9ad88747c68f00c0360cd7a9c0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eb9b1a2490974a1b9eb7031f7a0de62a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/15383 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cd54ff2fc9e94a9182aa5cede3a797c9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1922 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6138053ae2574bef9cd3bcb3250ff598"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1924 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cec268b3f1384eb196f9dc6852f35c1e"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Load BERT model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
    "#BERT-base (12-layer, 768-hidden, 12-heads)\n",
    "\n",
    "# Define metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids                   # True labels [0, 1, 2, ...]\n",
    "    preds = np.argmax(pred.predictions, axis=1)  # Predicted classes\n",
    "    acc = accuracy_score(labels, preds)       # Overall accuracy\n",
    "    f1 = f1_score(labels, preds, average='weighted')  # Class-balanced F1\n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138,
     "referenced_widgets": [
      "3e2b33fa17534b8187f2f52341dfcf43",
      "73d9b792268e4845af3761d30d2a30a0",
      "e716bf555b8e4ac9a4ec3c6333ea2b8e",
      "146c5e7bf81a43568e774cda605001a2",
      "d34b37fa4cba4e08876dca276c18212f",
      "71ca87f67d0244f0a74f179d33c0f76c",
      "bdbb284627cd4eb5893fbc053fc7260c",
      "6a9f9d042a044eb285b4151cf9f13da6",
      "93481aa1866e47ebbd4cbac851b4f120",
      "00849cd1269b44daa5f03b13c1d787f1",
      "21028aaef7b8414da4c801aee0c5c607"
     ]
    },
    "id": "JjZM13OmVZRM",
    "outputId": "9335afde-74bc-47cd-b174-c8875bf6af49"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3e2b33fa17534b8187f2f52341dfcf43"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_results\",         # Directory for checkpoints\n",
    "    eval_strategy=\"epoch\",              # Evaluate after each epoch\n",
    "    save_strategy=\"epoch\",              # Save model every epoch\n",
    "    learning_rate=2e-5,                 # Standard LR for BERT fine-tuning\n",
    "    per_device_train_batch_size=16,     # Batch size during training\n",
    "    per_device_eval_batch_size=64,      # Larger batch for evaluation\n",
    "    num_train_epochs=5,                 # Training duration\n",
    "    weight_decay=0.01,                  # L2 regularization strength\n",
    "    logging_dir=\"./bert_logs\",          # TensorBoard log location\n",
    "    logging_steps=10,                   # Log metrics every 10 steps\n",
    "    load_best_model_at_end=True,        # Keep best model checkpoint\n",
    "    metric_for_best_model=\"f1\",         # Model selection metric\n",
    "    greater_is_better=True,             # Higher F1 = better\n",
    "    report_to=\"none\",                   # Disable third-party logging\n",
    ")"
   ],
   "metadata": {
    "id": "b3vGv18HXBwj"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IexiiwlKXSFt",
    "outputId": "ffab8a82-1062-4afc-b306-05e65b759fda"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-10-01d28cc5ee29>:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "id": "eZvXXYUZXVOT",
    "outputId": "10f08262-40d0-45bd-ed3a-9f0742eb90a4"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4810, training_loss=0.4847017690942094, metrics={'train_runtime': 1768.0063, 'train_samples_per_second': 43.504, 'train_steps_per_second': 2.721, 'total_flos': 5059342131137280.0, 'train_loss': 0.4847017690942094, 'epoch': 5.0})"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "trainer.save_model(\"/content/drive/MyDrive/hate/vanilla_bert1\")\n",
    "tokenizer.save_pretrained(\"/content/drive/MyDrive/hate/vanilla_bert1\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4covv21teQxm",
    "outputId": "4f1f9f93-a3bf-49db-fecc-044ddb4c8431"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('/content/drive/MyDrive/hate/vanilla_bert1/tokenizer_config.json',\n",
       " '/content/drive/MyDrive/hate/vanilla_bert1/special_tokens_map.json',\n",
       " '/content/drive/MyDrive/hate/vanilla_bert1/vocab.txt',\n",
       " '/content/drive/MyDrive/hate/vanilla_bert1/added_tokens.json',\n",
       " '/content/drive/MyDrive/hate/vanilla_bert1/tokenizer.json')"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install datasets"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3MLNNLvoC-Pd",
    "outputId": "f7fdb89c-3066-4fed-8143-7947df13658f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "# Get the predictions and labels\n",
    "predictions = trainer.predict(eval_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "labels = predictions.label_ids\n",
    "\n",
    "# Classification Report\n",
    "report = classification_report(labels, preds, target_names=[\"Hateful\", \"Neither\", \"Offensive\"], output_dict=True)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(labels, preds, target_names=[\"Hateful\", \"Neither\", \"Offensive\"]))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(labels, preds)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Hateful\", \"Neither\", \"Offensive\"], yticklabels=[\"Hateful\", \"Neither\", \"Offensive\"])\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Optional: Save the classification report as a dictionary (for later use)\n",
    "# You can access precision, recall, f1 score, etc., from the report dictionary\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 755
    },
    "id": "Yie3AB0eel3L",
    "outputId": "86672fc4-7158-450c-91ba-1326e6e800fe"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Hateful       0.75      0.81      0.78       593\n",
      "     Neither       0.74      0.73      0.74       781\n",
      "   Offensive       0.57      0.54      0.56       548\n",
      "\n",
      "    accuracy                           0.70      1922\n",
      "   macro avg       0.69      0.69      0.69      1922\n",
      "weighted avg       0.70      0.70      0.70      1922\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install datasets"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mYmgGQoQpo30",
    "outputId": "3bf20a7f-1ea3-4b57-f3e8-e412e015f5e8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm import tqdm\n",
    "import re, numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ],
   "metadata": {
    "id": "UmCd3UqofxbO",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "83f8a5ca-5dc6-41f9-cdc0-684113a8861e"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "w-Zlef-colCe",
    "outputId": "6f0b6d9b-c95e-4099-f135-38d59d2c2c83"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "torch",
         "torchgen"
        ]
       },
       "id": "3ab9c9c9419a4c2eabfdf5b8ce93cc7f"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Load HateXplain dataset from HuggingFace Hub\n",
    "# trust_remote_code=True allows execution of dataset-specific loading scripts\n",
    "dataset = load_dataset(\"Hate-speech-CNERG/hatexplain\", trust_remote_code=True)\n",
    "\n",
    "# Convert token lists to full text strings\n",
    "def tokens_to_text(example):\n",
    "    \"\"\"Joins pre-tokenized text into continuous strings\"\"\"\n",
    "    return {\"text\": \" \".join(example[\"post_tokens\"])}  # Example: [\"This\", \"is\"] → \"This is\"\n",
    "\n",
    "# Aggregate multiple annotator labels via majority voting\n",
    "def get_majority_label(example):\n",
    "    \"\"\"Determines consensus label from annotators\"\"\"\n",
    "    label_list = example[\"annotators\"][\"label\"]  # List of annotations (e.g., [0, 1, 1])\n",
    "    majority_label = Counter(label_list).most_common(1)[0][0]  # Gets most frequent label\n",
    "    return {\"label\": majority_label}  # Returns single label\n",
    "\n",
    "# Apply preprocessing to all splits (train/validation/test)\n",
    "for split in dataset:\n",
    "    dataset[split] = dataset[split].map(tokens_to_text)  # First convert tokens → text\n",
    "    dataset[split] = dataset[split].map(get_majority_label)  # Then aggregate labels"
   ],
   "metadata": {
    "id": "SGQYTnbqsBsS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 3. Build vocab\n",
    "# Initialize vocabulary with special tokens\n",
    "word2idx = {\"<pad>\": 0, \"<unk>\": 1}  # Padding and unknown tokens\n",
    "idx = 2  # Next available index\n",
    "\n",
    "# Build vocabulary from training set\n",
    "for example in dataset[\"train\"]:\n",
    "    for word in better_tokenizer(example[\"text\"]):  # Process each token\n",
    "        if word not in word2idx:  # Only add new words\n",
    "            word2idx[word] = idx\n",
    "            idx += 1\n",
    "\n",
    "vocab_size = len(word2idx)  # Total unique tokens\n",
    "\n",
    "# 4. Load GloVe\n",
    "def load_glove(path, embedding_dim=100):\n",
    "    \"\"\"Loads pre-trained GloVe vectors from file\"\"\"\n",
    "    glove = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            word = parts[0]  # The word\n",
    "            vec = np.array(parts[1:], dtype=np.float32)  # Its vector\n",
    "            glove[word] = vec\n",
    "    return glove\n",
    "\n",
    "# Load pre-trained embeddings\n",
    "glove = load_glove(\"/content/drive/MyDrive/hate/glove.6B.100d.txt\")\n",
    "\n",
    "# Initialize embedding matrix (vocab_size × 100)\n",
    "embedding_matrix = np.random.normal(\n",
    "    scale=0.6,  # Standard deviation for random init\n",
    "    size=(vocab_size, embedding_dim)\n",
    ")\n",
    "\n",
    "# Map vocabulary to GloVe vectors\n",
    "for word, idx in word2idx.items():\n",
    "    if word in glove:\n",
    "        embedding_matrix[idx] = glove[word]  # Use pre-trained vector\n",
    "    # Else: keeps random initialization\n"
   ],
   "metadata": {
    "id": "_jXidORzsDa9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def encode(text, max_len=100):\n",
    "    # 1. Tokenization\n",
    "    tokens = better_tokenizer(text)  # Applies your advanced tokenizer\n",
    "\n",
    "    # 2. Vocabulary Lookup\n",
    "    ids = [word2idx.get(t, word2idx[\"<unk>\"]) for t in tokens]  # Word → ID\n",
    "\n",
    "    # 3. Length Normalization\n",
    "    ids = ids[:max_len]  # Truncate if exceeds max_len\n",
    "\n",
    "    # 4. Padding\n",
    "    ids += [word2idx[\"<pad>\"]] * (max_len - len(ids))  # Pad if shorter\n",
    "\n",
    "    # 5. Tensor Conversion\n",
    "    return torch.tensor(ids)  # Convert to PyTorch tensor"
   ],
   "metadata": {
    "id": "pWLtwO0Js9ka"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 6. Dataset\n",
    "class HateXplainCNNData(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data  #  dataset split\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)  # Required for DataLoader\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx][\"text\"]  # Raw text\n",
    "        label = self.data[idx][\"label\"]  # 0,1,2 (normal/hate/offensive)\n",
    "        return encode(text), torch.tensor(label)  # (padded_ids, label)\n",
    "\n",
    "train_dataset = HateXplainCNNData(dataset[\"train\"])\n",
    "val_dataset = HateXplainCNNData(dataset[\"validation\"])\n",
    "test_dataset = HateXplainCNNData(dataset[\"test\"])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n"
   ],
   "metadata": {
    "id": "ySLjLts6tAMO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 7. CNN Model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes, embedding_weights, kernel_sizes=[3,4,5], num_filters=128):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_weights, dtype=torch.float), freeze=False, padding_idx=0)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embed_dim, out_channels=num_filters, kernel_size=k)\n",
    "            for k in kernel_sizes\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * num_filters, num_classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # 1. Embedding lookup\n",
    "    x = self.embedding(x)          # [32, 100, 100]\n",
    "\n",
    "    # 2. Rearrange for Conv1D (channels first)\n",
    "    x = x.permute(0, 2, 1)         # [32, 100, 100]\n",
    "\n",
    "    # 3. Parallel convolution + ReLU\n",
    "    x = [torch.relu(conv(x)) for conv in self.convs]\n",
    "    # Now a list of [32, 128, seq_len-k+1] tensors\n",
    "\n",
    "    # 4. Global max pooling\n",
    "    x = [torch.max(c, dim=2)[0] for c in x]\n",
    "    # List of [32, 128] tensors\n",
    "\n",
    "    # 5. Concatenate features\n",
    "    x = torch.cat(x, dim=1)        # [32, 384] (3*128)\n",
    "\n",
    "    # 6. Dropout for regularization\n",
    "    x = self.dropout(x)            # Randomly zero 30% of units\n",
    "\n",
    "    # 7. Final classification\n",
    "    return self.fc(x)              # [32, 3] (class logits)"
   ],
   "metadata": {
    "id": "-hUr0XSstA2N"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 8. Training Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleCNN(\n",
    "    vocab_size=vocab_size,          # From your vocabulary\n",
    "    embed_dim=embedding_dim,        # 100 (matches GloVe)\n",
    "    num_classes=3,                  # Normal/Hate/Offensive\n",
    "    embedding_weights=embedding_matrix  # Your GloVe-enhanced matrix\n",
    ").to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()#Standard for multi-class classification\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='max',      # Monitor F1 (higher is better)\n",
    "    factor=0.5,      # Halve LR when plateauing\n",
    "    patience=1       # Wait 1 epoch before reducing\n",
    ")\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(y.cpu().tolist())\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "    return acc, f1"
   ],
   "metadata": {
    "id": "alnbSOAYtFXE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "best_f1 = 0  # Track best validation F1 for model selection\n",
    "\n",
    "for epoch in range(20):\n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        # 1. Forward Pass\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "\n",
    "        # 2. Loss Calculation (with class weighting)\n",
    "        loss = loss_fn(logits, y)\n",
    "\n",
    "        # 3. Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient Clipping (prevents exploding gradients)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # 4. Weight Update\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Evaluation Phase\n",
    "    val_acc, val_f1 = evaluate(model, val_loader)\n",
    "\n",
    "    # Print Metrics\n",
    "    print(f\"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f} | \"\n",
    "          f\"Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f}\")\n",
    "\n",
    "    # Learning Rate Scheduling\n",
    "    scheduler.step(val_f1)  # Adjusts LR based on F1 plateau\n",
    "\n",
    "    # Model Checkpointing\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_f1': val_f1,\n",
    "        }, \"best_model.pt\")\n",
    "        print(f\" Best model saved with F1: {val_f1:.4f}\")\n",
    "\n",
    "    # Early Stopping (optional)\n",
    "    if epoch > 5 and val_f1 < best_f1 - 0.02:\n",
    "        print(\" Early stopping triggered!\")\n",
    "        break\n",
    "\n",
    "# Final Model Save\n",
    "torch.save(model.state_dict(), \"/content/drive/MyDrive/hate/cnn-hate-speech-model_updated1.pt\")\n",
    "print(\"Model saved to Google Drive!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H3OQ6kNjtJ0l",
    "outputId": "8a040447-254d-4c80-e865-ffc4d88789cd"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 1: 100%|██████████| 481/481 [00:11<00:00, 41.51it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1 | Loss: 63.9584 | Val Acc: 0.6379 | Val F1: 0.6341\n",
      " Best model saved.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 2: 100%|██████████| 481/481 [00:12<00:00, 39.46it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 2 | Loss: 46.1262 | Val Acc: 0.6311 | Val F1: 0.6303\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 3: 100%|██████████| 481/481 [00:11<00:00, 41.07it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 3 | Loss: 39.9253 | Val Acc: 0.6186 | Val F1: 0.6170\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 4: 100%|██████████| 481/481 [00:11<00:00, 41.36it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 4 | Loss: 32.1033 | Val Acc: 0.6228 | Val F1: 0.6232\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 5: 100%|██████████| 481/481 [00:11<00:00, 40.10it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 5 | Loss: 29.0189 | Val Acc: 0.6238 | Val F1: 0.6228\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 6: 100%|██████████| 481/481 [00:11<00:00, 41.71it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 6 | Loss: 26.2203 | Val Acc: 0.6264 | Val F1: 0.6236\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 7: 100%|██████████| 481/481 [00:11<00:00, 42.21it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 7 | Loss: 25.2146 | Val Acc: 0.6181 | Val F1: 0.6195\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 8: 100%|██████████| 481/481 [00:11<00:00, 41.33it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 8 | Loss: 24.0162 | Val Acc: 0.6233 | Val F1: 0.6223\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 9: 100%|██████████| 481/481 [00:12<00:00, 39.37it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 9 | Loss: 22.6042 | Val Acc: 0.6228 | Val F1: 0.6214\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 10: 100%|██████████| 481/481 [00:11<00:00, 40.81it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 10 | Loss: 22.1647 | Val Acc: 0.6238 | Val F1: 0.6220\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 11: 100%|██████████| 481/481 [00:11<00:00, 42.01it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 11 | Loss: 22.0496 | Val Acc: 0.6238 | Val F1: 0.6233\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 12: 100%|██████████| 481/481 [00:11<00:00, 42.19it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 12 | Loss: 21.5228 | Val Acc: 0.6233 | Val F1: 0.6220\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 13: 100%|██████████| 481/481 [00:11<00:00, 43.01it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 13 | Loss: 20.8715 | Val Acc: 0.6238 | Val F1: 0.6234\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 14: 100%|██████████| 481/481 [00:11<00:00, 42.28it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 14 | Loss: 20.7930 | Val Acc: 0.6243 | Val F1: 0.6232\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 15: 100%|██████████| 481/481 [00:11<00:00, 42.30it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 15 | Loss: 20.7387 | Val Acc: 0.6249 | Val F1: 0.6232\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 16: 100%|██████████| 481/481 [00:11<00:00, 41.10it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 16 | Loss: 20.7374 | Val Acc: 0.6249 | Val F1: 0.6243\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 17: 100%|██████████| 481/481 [00:12<00:00, 39.47it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 17 | Loss: 20.7671 | Val Acc: 0.6233 | Val F1: 0.6223\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 18: 100%|██████████| 481/481 [00:11<00:00, 41.38it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 18 | Loss: 20.9701 | Val Acc: 0.6228 | Val F1: 0.6218\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 19: 100%|██████████| 481/481 [00:13<00:00, 36.00it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 19 | Loss: 21.1288 | Val Acc: 0.6223 | Val F1: 0.6211\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 20: 100%|██████████| 481/481 [00:12<00:00, 39.49it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 20 | Loss: 20.4721 | Val Acc: 0.6228 | Val F1: 0.6216\n",
      "Model saved to Google Drive!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the best saved model\n",
    "model.load_state_dict(torch.load(\"/content/drive/MyDrive/hate/cnn-hate-speech-model_updated1.pt\"))\n",
    "model.eval()\n",
    "\n",
    "# Evaluation on test set\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "        all_labels.extend(y.cpu().tolist())\n",
    "\n",
    "# Generate classification report\n",
    "target_names = ['Normal', 'Offensive', 'Hate']  # Change as per label mapping if needed\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(all_labels, all_preds, target_names=target_names, digits=4))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 715
    },
    "id": "28t2mfd8txGW",
    "outputId": "4c26f08c-af6b-4b76-b00f-b2c356271493"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal     0.6746    0.7189    0.6960       594\n",
      "   Offensive     0.6889    0.6880    0.6884       782\n",
      "        Hate     0.4647    0.4325    0.4480       548\n",
      "\n",
      "    accuracy                         0.6247      1924\n",
      "   macro avg     0.6094    0.6131    0.6108      1924\n",
      "weighted avg     0.6206    0.6247    0.6223      1924\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Install ftfy for fixing text encoding issues\n",
    "!pip install -q ftfy\n",
    "\n",
    "import pandas as pd\n",
    "from ftfy import fix_text\n",
    "\n",
    "# Load the previously uploaded CSV file\n",
    "input_path = \"/content/drive/MyDrive/hate/data/test_data_test.csv\"\n",
    "output_path_cleaned = \"/content/drive/MyDrive/hate/data/test_data_cleaned.csv\"\n",
    "\n",
    "# Read CSV (attempt auto-detection of encoding and fix common issues)\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# Function to clean text: fix encoding, normalize whitespace, lowercase\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        fixed = fix_text(text)  # Fix mojibake and encoding\n",
    "        fixed = fixed.lower()   # Lowercase\n",
    "        fixed = ' '.join(fixed.split())  # Normalize whitespace\n",
    "        return fixed\n",
    "    return text\n",
    "\n",
    "# Apply cleaning to all string columns\n",
    "df_cleaned = df.applymap(clean_text)\n",
    "\n",
    "# Save cleaned CSV\n",
    "df_cleaned.to_csv(output_path_cleaned, index=False, encoding='utf-8')\n",
    "\n",
    "output_path_cleaned\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "Hw-kJSLz5Mcl",
    "outputId": "4ee0d908-87dc-4358-fe09-caaaf37db7fe"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-5-9cdc9d3cd8bf>:24: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_cleaned = df.applymap(clean_text)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/content/drive/MyDrive/hate/data/test_data_cleaned.csv'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 5
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/hate/data/test_data_cleaned.csv\")\n",
    "# Show basic info\n",
    "print(df.columns)\n",
    "print(df.head())\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6XgDMNSI5jxE",
    "outputId": "7135bdda-1a7e-4c1c-9cbe-73256110104e"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Index(['original_text', 'neutral_text'], dtype='object')\n",
      "                             original_text  \\\n",
      "0                     that's so gay, dude.   \n",
      "1  trans people are just confused weirdos.   \n",
      "2              can't wait for the weekend!   \n",
      "3      acting like a total spaz right now.   \n",
      "4         women are too emotional to lead.   \n",
      "\n",
      "                                        neutral_text  \n",
      "0        that's a bit over-the-top, don't you think?  \n",
      "1  it's important to respect everyone's identity,...  \n",
      "2         looking forward to some rest this weekend!  \n",
      "3    they're being very energetic and unpredictable.  \n",
      "4  i think leadership should be based on individu...  \n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Rename columns for clarity\n",
    "df = df.rename(columns={\"Offensive\": \"original_text\", \"Non-Offensive\": \"neutral_text\"})\n",
    "\n",
    "# Drop unused columns\n",
    "df = df[[\"original_text\", \"neutral_text\"]].dropna()\n",
    "\n",
    "# Preview cleaned data\n",
    "df.head()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "kfmvlNEe5viL",
    "outputId": "8b58682a-89a9-4bb7-abba-238722f72cf9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install transformers datasets sentencepiece --quiet\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BadDzgwl55g-",
    "outputId": "9b7b6f25-e234-41c7-f6f3-66736fcb00b4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load cleaned data\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/hate/data/test_data_cleaned.csv\")[[\"original_text\", \"neutral_text\"]] # Use the new column names: 'original_text', 'neutral_text'\n",
    "\n",
    "# Optional: Add task prefix\n",
    "df[\"original_text\"] = \"rephrase: \" + df[\"original_text\"]\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "dataset = Dataset.from_pandas(df[[\"original_text\", \"neutral_text\"]])\n",
    "dataset = dataset.train_test_split(test_size=0.1)  # 90% train, 10% test"
   ],
   "metadata": {
    "id": "r4Gr5Qt36ARF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"original_text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "def tokenize_labels(batch):\n",
    "    labels = tokenizer(batch[\"neutral_text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    batch[\"labels\"] = labels[\"input_ids\"]\n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(tokenize)\n",
    "dataset = dataset.map(tokenize_labels)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345,
     "referenced_widgets": [
      "77c34bf484b94daabadb4cc6d858eec5",
      "93d6f90b9f9c4ba88400919ba09b8bfd",
      "fccf996c01794d23aaaadc9ad7803ade",
      "197e1226448a41dd9cfaa3967fb4fa76",
      "511fc7659c7343e1a26474e93b8c8a6e",
      "3557144bd7d94adf9eb1db09d32efec6",
      "ed9c648e424140679a44ef41756401f3",
      "647e68c93d9c42fc8645a8ea9eaf7a19",
      "bffb3649fa0149bf9937dd5c875024fa",
      "2edc2d9c9c5b4b9f8b5ba73e856d7a33",
      "2c8d796bdf6744819c74e4e36e774bef",
      "913f2d48be914bd0ba2ee23b0ecd171f",
      "eed4c599f7ce4bbb9ba04ec3ce0b06a6",
      "c546376c4bfb41c7a799031ba818f5d3",
      "1545fdbe734c4eb6b806672a923ac421",
      "39a43f6f0dda473596776bbea8f66516",
      "c91559bd4bbe4e4da6c690ef2ef154b2",
      "3ade68cc3afb4b8b85b2558a0e6bd080",
      "409b0918ed0347be83ed45a5d737d5bf",
      "b5dd06cb96fa4454b4abc41a1c251179",
      "c78cb80ae600451eaa7021a16e96cb45",
      "f4acc4e57f58423cad41fd22748f9764",
      "d77f673cfdea4c799132aa1bb689e1ca",
      "667626e230a24f3dbf3bb868961cf417",
      "c7b45cf8c5b3450ead6198a45b496433",
      "06b70d7ed1b048dfb040b84f8845fee9",
      "93ce01fa10014f2f8c71db086991bc55",
      "af452950d72a4e7196f5fcc5a8686ed1",
      "ac448ade08b64a908f92f0f38a226025",
      "75fc737c2d1c43e9b61f95b56fe25aeb",
      "0a97964a05104d69bbf8398f5ee55d00",
      "43ff75affeef4060a200f215ba7dd018",
      "0cac19c3d4dd46168c55f20ef4c0a1ce",
      "8c28bdf650e8402786c651d7fc24ca5f",
      "5cf155e859df4e05944ee1086bcb62ce",
      "c5f4bec43122477caef4a9014852f4ac",
      "8f5b213ad892435592a21644083e3f66",
      "de9405e1a7dd4e5e81b2acceaf6c5f8f",
      "afdd9cf9534b41b59597089cac8315da",
      "3a8f3c1b52f945279284206232a6faff",
      "2be89645b4b14acf93876919a6f181a7",
      "9b07155d4ce8457e9528958fb7321573",
      "b200050efca5460ab76efd010bede868",
      "2ebd2ad2fef9475795ec48a0e041edee",
      "9a3d204aa33d4fc4bfabc0e6c8837aa7",
      "c54d9bd2260f465a8eb5dfb7cda87668",
      "e5144bd0aef24edaa220ea893275bca9",
      "90cbf1a82ad24a178ea679a95a05a5bb",
      "94c083e7dc8d41d495c0bf80816c12d1",
      "ef085599e8e04217be1476caa15486b1",
      "5d9ce4a133bc42319d64d0ba1a73076b",
      "3fa38dc6986e47bf8a81cc9fbba214ad",
      "992ad9723d9d4f3a9f31297a0741e6fb",
      "aee7d85b3b624e458371fbc64cdc1650",
      "17ad4009cbfb48f486a2919d5d15d83e",
      "2db71cdab95e4b2dbe0f3a209cd74343",
      "2f63be35623b45d190f718a3179f5085",
      "8ac1180e844e46ddacbfe671c8ae159f",
      "6a6ad73265634f6ca58f282f2806ec67",
      "055e5e3b97034eab8591ac95ab1a0e6e",
      "c2f8f1030aa14197ada5263b3ad6f2d2",
      "c6b3bc4ced35416d9eda7ddc2da5c03d",
      "6edc53c401084cdd837e7fb37ee700c8",
      "deea0fc5829442d3b111e24f9ad41423",
      "dc11f5576b9349a4bd16d40ee0d4d004",
      "a73802c7209b469eae8751c08022ee93",
      "75a1db62174b4710bdee8fc6c56f3876",
      "8d730a94199e4bce8f56bf0afd725ac0",
      "d19529c485864338b277b8fdb056ca21",
      "a226534e10f540c5b14b521515cdbbe6",
      "99bfd1ae27e84eaf80f9b935b0c9d897",
      "6c100340ecaa4f4cb9fb5562bb8d08fc",
      "3220f2d5ff984997b1367879ec4b8ccf",
      "661984a402994ac1b8352274fc907227",
      "0a866e7d041c42b2bc0f82c08e35c026",
      "a787372d1b124f1988e2a5a1e9b9a525",
      "30dde9e3f6ac4e14bc9625403e598d40"
     ]
    },
    "id": "q7CmeyJI6IBq",
    "outputId": "4ad84c8f-fb6f-4f61-b7b7-a16cd4938572"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "77c34bf484b94daabadb4cc6d858eec5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "913f2d48be914bd0ba2ee23b0ecd171f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d77f673cfdea4c799132aa1bb689e1ca"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/9018 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8c28bdf650e8402786c651d7fc24ca5f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1002 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9a3d204aa33d4fc4bfabc0e6c8837aa7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/9018 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2db71cdab95e4b2dbe0f3a209cd74343"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1002 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "75a1db62174b4710bdee8fc6c56f3876"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, TrainingArguments, Trainer\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./t5_rephrase_model\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504,
     "referenced_widgets": [
      "273b0c0bc0be4648aacc05229deee8ad",
      "69cbf326ffdf4352b14e1ae9df126ffc",
      "0565921639be4d62842b5e59e5f81f8e",
      "67886eb5a6d04bfcaeccbf1a3816b879",
      "5578a8fb77e74212af5f45aed1540336",
      "090f6680501c4720a36f4ff183160d5e",
      "88ff990dc868426594b34a71afa11215",
      "db7e6e082b4941c2ac1e3de5e1d9daf7",
      "d571d47652584745935f0ce6325afe17",
      "515406e06d89405e9d3c8fc0b5a241e4",
      "643eea08aa094dfdb42c2fca2ad29174",
      "b80b25767ade47b1a9b2abde6fc56dc6",
      "f4721d68e3f54223be68c9da70264e15",
      "42ee4e625e934faa8f1465ddd6786343",
      "173f04bfd71a4d8b9873a0cfce36c5a3",
      "141035451fb540eaa962c5f34adfce75",
      "a2cadc8c1db24526b01ddc5f94b93ad4",
      "d481d41aff1b4220a4811dca3d41c03f",
      "d276b4eb76b940a4aa58aaf2808f3b10",
      "5be06ca11f0a4d07bee49c1f35598950",
      "a6ebb77afc9147c1afd73c24c007b336",
      "740feeb204654f429f6b41ed96faa526",
      "fae127dd744148919b24d4a671609131",
      "01dbe99a100343659f43c7a571ef2e85",
      "ec00e05e7cf145799284f144431e8789",
      "efe5ea728f484fbaba39378ca480b15f",
      "8e8cbe147b8348288a48ccb4a0485d3e",
      "78ec7bb7c4b9433cb68deb3021ccd831",
      "d05946e46b9841e581df9d3d3d42c2b5",
      "1dccaaa4fb2f46758e6a67adc00fa5f8",
      "aa23b27717d44f9d916a7f52d5e03c9f",
      "3e6e7e834ef9432cb030c941a4144553",
      "f1d7ae3804db4625bae8aaf545f2028d"
     ]
    },
    "id": "lyc8h1eX6dR0",
    "outputId": "8c8caeb8-03d4-45fd-cd12-6af120a15478"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "273b0c0bc0be4648aacc05229deee8ad"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b80b25767ade47b1a9b2abde6fc56dc6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fae127dd744148919b24d4a671609131"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "<ipython-input-14-de741f3d0c40>:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainOutput(global_step=11275, training_loss=0.031040717153691225, metrics={'train_runtime': 1239.3583, 'train_samples_per_second': 36.382, 'train_steps_per_second': 9.097, 'total_flos': 1525640457093120.0, 'train_loss': 0.031040717153691225, 'epoch': 5.0})"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "trainer.save_model(\"/content/drive/MyDrive/hate/t5-rephrase-model1\")\n",
    "tokenizer.save_pretrained(\"/content/drive/MyDrive/hate/t5-rephrase-model1\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sPp0loqV6mz2",
    "outputId": "0d6cc73e-c392-438b-e4cf-1b396ece3dce"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('/content/drive/MyDrive/hate/t5-rephrase-model1/tokenizer_config.json',\n",
       " '/content/drive/MyDrive/hate/t5-rephrase-model1/special_tokens_map.json',\n",
       " '/content/drive/MyDrive/hate/t5-rephrase-model1/spiece.model',\n",
       " '/content/drive/MyDrive/hate/t5-rephrase-model1/added_tokens.json',\n",
       " '/content/drive/MyDrive/hate/t5-rephrase-model1/tokenizer.json')"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#T5 output\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/hate/data/test_data_cleaned.csv\")\n",
    "df = df.rename(columns={\"Offensive\": \"original_text\", \"Non-Offensive\": \"neutral_text\"})\n",
    "df = df[[\"original_text\", \"neutral_text\"]].dropna()\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_path = \"/content/drive/MyDrive/hate/t5-rephrase-model1\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "model.eval()\n",
    "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Prepare inputs\n",
    "BATCH_SIZE = 16\n",
    "inputs = [f\"rephrase: {t}\" for t in df[\"original_text\"].tolist()]\n",
    "all_preds = []\n",
    "\n",
    "# Batched inference\n",
    "for i in tqdm(range(0, len(inputs), BATCH_SIZE)):\n",
    "    batch = inputs[i:i+BATCH_SIZE]\n",
    "    encodings = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    input_ids = encodings[\"input_ids\"].to(model.device)\n",
    "    attention_mask = encodings[\"attention_mask\"].to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=128,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    decoded = [tokenizer.decode(o, skip_special_tokens=True) for o in outputs]\n",
    "    all_preds.extend(decoded)\n",
    "\n",
    "# Heuristic label function (same as before)\n",
    "def categorize(text):\n",
    "    text = text.lower()\n",
    "    if any(w in text for w in [\"n****\", \"chink\", \"race\", \"ethnicity\"]):\n",
    "        return \"Hateful\"\n",
    "    elif any(w in text for w in [\"retarded\", \"idiot\", \"hoe\"]):\n",
    "        return \"Offensive\"\n",
    "    else:\n",
    "        return \"Neither\"\n",
    "\n",
    "def categorize_pred(text):\n",
    "    text = text.lower()\n",
    "    if any(w in text for w in [\"race\", \"ethnicity\", \"n****\", \"chink\"]):\n",
    "        return \"Hateful\"\n",
    "    elif any(w in text for w in [\"respect\", \"tone\", \"calm\", \"inclusive\"]):\n",
    "        return \"Offensive\"\n",
    "    else:\n",
    "        return \"Neither\"\n",
    "\n",
    "# Assign labels\n",
    "y_true = df[\"original_text\"].apply(categorize).tolist()\n",
    "y_pred = [categorize_pred(p) for p in all_preds]\n",
    "\n",
    "# Evaluation\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print(f\"\\nAccuracy: {acc:.2f}\\n\")\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_true, y_pred, digits=2))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[\"Hateful\", \"Offensive\", \"Neither\"])\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\",\n",
    "            xticklabels=[\"Hateful\", \"Offensive\", \"Neither\"],\n",
    "            yticklabels=[\"Hateful\", \"Offensive\", \"Neither\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 829
    },
    "id": "Ij7Y_edr6uIE",
    "outputId": "9493ece2-2e7a-4831-86f3-93d644528409"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Accuracy: 0.90\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Hateful       0.00      0.00      0.00         0\n",
      "     Neither       1.00      0.90      0.95     10020\n",
      "   Offensive       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.90     10020\n",
      "   macro avg       0.33      0.30      0.32     10020\n",
      "weighted avg       1.00      0.90      0.95     10020\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install gradio transformers --quiet\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NNF2zbMFLfkb",
    "outputId": "2f54bb5e-c519-4e45-baa9-43d2cb7bf02c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import gradio as gr\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load hate speech classifier\n",
    "print(\" Loading classifier model...\")\n",
    "clf_model_path = \"/content/drive/MyDrive/hate/roberta-hate-speech-model-20250501T060330Z-001/roberta-hate-speech-model\"\n",
    "clf_tokenizer = AutoTokenizer.from_pretrained(clf_model_path)\n",
    "clf_model = AutoModelForSequenceClassification.from_pretrained(clf_model_path)\n",
    "print(\" Classifier loaded!\")\n",
    "\n",
    "# Load rephraser model\n",
    "print(\"Loading rephraser model...\")\n",
    "reph_model_path = \"/content/drive/MyDrive/hate/t5-rephrase-model1\"\n",
    "reph_tokenizer = AutoTokenizer.from_pretrained(reph_model_path)\n",
    "reph_model = AutoModelForSeq2SeqLM.from_pretrained(reph_model_path)\n",
    "print(\"Rephraser loaded!\")\n",
    "\n",
    "label_map = {0: \"Hateful\", 1: \"Neither\", 2: \"Offensive\"}\n",
    "\n",
    "def classify_and_rephrase(text, rephrase_option):\n",
    "    # Step 1: Classification\n",
    "    inputs = clf_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = clf_model(**inputs)\n",
    "        probs = F.softmax(outputs.logits, dim=1)\n",
    "        confidence, prediction = torch.max(probs, dim=1)\n",
    "\n",
    "    label_str = label_map[prediction.item()]\n",
    "    confidence_pct = round(confidence.item() * 100, 2)\n",
    "    prob_str = \", \".join(f\"{label_map[i]}: {round(p * 100, 2)}%\" for i, p in enumerate(probs[0].tolist()))\n",
    "\n",
    "    response = f\" Prediction:{label_str}\\t Confidence: {confidence_pct}%\\t Probabilities: {prob_str}\"\n",
    "\n",
    "    # Step 2: Optional Rephrasing\n",
    "    if rephrase_option and label_str in [\"Hateful\", \"Offensive\"]:\n",
    "        reph_input = reph_tokenizer(f\"rephrase: {text}\", return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        reph_output = reph_model.generate(\n",
    "            **reph_input,\n",
    "            max_length=64,     # Reduced for speed\n",
    "            num_beams=2,       # Reduced for speed\n",
    "            early_stopping=True\n",
    "        )\n",
    "        rephrased_text = reph_tokenizer.decode(reph_output[0], skip_special_tokens=True)\n",
    "        response += f\"\\n Rephrased Text: {rephrased_text}\"\n",
    "\n",
    "    return response\n",
    "\n",
    "# Launch Gradio UI\n",
    "gr.Interface(\n",
    "    fn=classify_and_rephrase,\n",
    "    inputs=[\n",
    "        gr.Textbox(lines=3, placeholder=\"Enter a sentence to classify...\"),\n",
    "        gr.Checkbox(label=\"Also rephrase if hateful or offensive\")\n",
    "    ],\n",
    "    outputs=\"markdown\",\n",
    "    title=\" Hate Speech Classifier + Rephraser\",\n",
    "    description=\"Classifies text as Hateful, Offensive, or Neither. Optionally rephrases hateful/offensive text into neutral language.\",\n",
    ").launch()  # Remove share=True for speed\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 715
    },
    "id": "IshhJL4N618Q",
    "outputId": "833af266-8f35-45d8-97a0-123c795590b2"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " Loading classifier model...\n",
      " Classifier loaded!\n",
      "Loading rephraser model...\n",
      "Rephraser loaded!\n",
      "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://3aac783c2a4882da8e.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div><iframe src=\"https://3aac783c2a4882da8e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "execution_count": 3
    }
   ]
  }
 ]
}